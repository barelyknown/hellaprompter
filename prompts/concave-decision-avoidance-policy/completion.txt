# Company Policy: Avoiding Concave Decisions (Low-Upside, High-Downside Risks)

For a company to be great and survive in the long run, it must adopt a **risk-conscious culture**. This policy defines “concave decisions” – choices with low upside and potentially high, uncertain downside – and provides guidance to **avoid exposing the company to catastrophic risks for minimal gain**. It applies to **all employees** and seeks to reinforce prudent decision-making as we continue to grow through acquisitions.

## TL;DR Checklist: Is This a Concave Decision?

Use this quick checklist before making key decisions or taking action. If **most answers are “Yes,” you’re likely facing a concave decision** and should reconsider or seek alternatives:

- **Disproportionate Downside?** – Is the *worst-case* outcome **much worse** than the *best-case* outcome is good? (Huge potential loss for a small potential gain) ([What is Optionality? How Can Optionality Improve My Performance?](https://taylorpearson.me/optionality/#:~:text=Concavity%20is%20the%20opposite%20of,then%20you%20lose%20a%20lot)). For example, *“Am I picking up pennies in front of a bulldozer?”* – gaining a little if all goes well, but risking a lot if it goes wrong ([What is Optionality? How Can Optionality Improve My Performance?](https://taylorpearson.me/optionality/#:~:text=Concavity%20is%20the%20opposite%20of,then%20you%20lose%20a%20lot)).  
- **Catastrophic or Irreversible?** – Could a failure lead to **irreversible damage or a catastrophe** (e.g. major outage, data loss, legal penalty) while success only gives a minor benefit? If the decision **risks the company’s survival or reputation for a trivial gain**, it’s concave. *(Never bet the firm for a small reward.)*  
- **Probability Blindness?** – Are you relying on the *low probability* of bad outcomes to justify the action? Remember that if an outcome’s downside is **unacceptable (ruinous)**, even a low probability is too high ([Principles by Ray Dalio](https://www.principles.com/principles/b98e3caf-b72f-4d4e-a0f7-b3470f437216/#:~:text=Make%20sure%20that%20the%20probability,is%20nil)). *In other words: “Make sure that the probability of the unacceptable (risk of ruin) is nil.”* ([Principles by Ray Dalio](https://www.principles.com/principles/b98e3caf-b72f-4d4e-a0f7-b3470f437216/#:~:text=Make%20sure%20that%20the%20probability,is%20nil))  
- **No Skin in the Game?** – Would you make the same decision if **you personally** suffered the downside? If not, be wary. Misaligned incentives (when decision-makers don’t bear consequences) often lead to concave risks ([Antifragile by Nassim Nicholas Taleb - Summary & Notes](https://www.grahammann.net/book-notes/antifragile-nassim-nicholas-taleb#:~:text=,is%20more%20and%C2%A0usually%20more%20effective)). Always imagine *you* “go down with the ship” ([Antifragile (book) - Wikipedia](https://en.wikipedia.org/wiki/Antifragile_(book)#:~:text=)) – it focuses the mind on truly unacceptable downsides.  
- **Safe Alternatives Ignored?** – Have you **skipped safer options or incremental tests** that cap downside? (E.g. not piloting a change on a small scale first.) If a more convex approach (low downside, high upside) exists, **pursue that instead** of a high-downside leap.  
- **Gut Check – Any Black Swans?** – If an unforeseen “Black Swan” event occurred (something highly unexpected), would this decision **blow up** disproportionately? Fragile (concave) plans break under volatility ([Nassim Nicholas Taleb: To Prevail in an Uncertain World, Get Convex - Articles - Advisor Perspectives](https://www.advisorperspectives.com/articles/2013/07/16/nassim-nicholas-taleb-to-prevail-in-an-uncertain-world-get-convex#:~:text=Convexity%20refers%20to%20the%20shape,we%20want%20to%20be%20convex)). Ask yourself, *“If things change suddenly or we’re wrong in our assumptions, what’s the damage?”* If the answer is “devastating,” avoid or redesign the action.

Use the above as a **personal checklist**. When in doubt, **pause and escalate** the decision to a manager or risk review committee. It’s always better to slow down than to charge into a potential disaster.

## Supporting Insights and Theory: Antifragility, Asymmetry, and Risk Culture

Understanding the theory behind this policy will help internalize why we avoid concave decisions. Our approach is inspired by Nassim Nicholas Taleb’s concepts of **antifragility**, along with principles from risk management, decision theory, and behavioral economics:

- **Convex vs. Concave Payoffs (Taleb’s Antifragility):** In Taleb’s framework, **fragile things have more downside than upside** from shocks, which he calls a *concave* payoff profile ([Antifragile (book) - Wikipedia](https://en.wikipedia.org/wiki/Antifragile_(book)#:~:text=stressor%20or%20source%20of%20harm,1)) ([Antifragile by Nassim Nicholas Taleb - Summary & Notes](https://www.grahammann.net/book-notes/antifragile-nassim-nicholas-taleb#:~:text=,is%20more%20and%C2%A0usually%20more%20effective)). By contrast, **antifragile things benefit from volatility** – a *convex* profile where upside outweighs downside ([Nassim Nicholas Taleb: To Prevail in an Uncertain World, Get Convex - Articles - Advisor Perspectives](https://www.advisorperspectives.com/articles/2013/07/16/nassim-nicholas-taleb-to-prevail-in-an-uncertain-world-get-convex#:~:text=Convexity%20refers%20to%20the%20shape,we%20want%20to%20be%20convex)) ([Antifragile by Nassim Nicholas Taleb - Summary & Notes](https://www.grahammann.net/book-notes/antifragile-nassim-nicholas-taleb#:~:text=,is%20more%20and%C2%A0usually%20more%20effective)). We want to position our decisions and strategies to be convex (or at least robust), **never concave**. *“Anything that has more upside than downside from random events is antifragile; the reverse is fragile.”* ([Antifragile by Nassim Nicholas Taleb - Summary & Notes](https://www.grahammann.net/book-notes/antifragile-nassim-nicholas-taleb#:~:text=,is%20more%20and%C2%A0usually%20more%20effective)) In practice, this means **setups where we are harmed much more by an error than we can benefit from a correct call are to be avoided** ([
Lead Time Distributions and Antifragility | Connected Knowledge](https://connected-knowledge.com/2014/09/09/lead-time-distributions-and-antifragility/#:~:text=A%20concave%20payoff%20function%20would,limited%20gains%20and%20unlimited%20losses)). Taleb uses the analogy of *convexity* like a financial option – limited loss if wrong, big gain if right – whereas concave exposures are like **“picking up pennies in front of a bulldozer”** ([What is Optionality? How Can Optionality Improve My Performance?](https://taylorpearson.me/optionality/#:~:text=Concavity%20is%20the%20opposite%20of,then%20you%20lose%20a%20lot)). Our policy institutionalizes this wisdom: seek decisions that **gain from uncertainty** or at least resist it, and shun those that **break under volatility** ([Nassim Nicholas Taleb: To Prevail in an Uncertain World, Get Convex - Articles - Advisor Perspectives](https://www.advisorperspectives.com/articles/2013/07/16/nassim-nicholas-taleb-to-prevail-in-an-uncertain-world-get-convex#:~:text=Convexity%20refers%20to%20the%20shape,we%20want%20to%20be%20convex)).

- **“Skin in the Game” – Accountability and Incentives:** Taleb famously notes that the **“largest source of fragility” is the *absence of skin in the game*** ([Antifragile by Nassim Nicholas Taleb - Summary & Notes](https://www.grahammann.net/book-notes/antifragile-nassim-nicholas-taleb#:~:text=,is%20more%20and%C2%A0usually%20more%20effective)). When individuals do not **personally bear the consequences** of their risks, they may take reckless or concave bets. This policy insists on a culture of **ownership and accountability**: we encourage asking, *“If I had to live with the worst outcome, would I still proceed?”* Decisions that pass off extreme downside to the company (or others) while offering only minor upside to the initiator are unethical and fragile ([Having 'Skin in the Game' - Common Dreams](https://www.commondreams.org/views/2012/12/07/having-skin-game#:~:text=In%20Washington%2C%20DC%20and%20on,power%20without%20responsibility%20or%20vulnerability)). In finance and history, lack of skin in the game (e.g. executives taking huge risks for bonus upside, leaving shareholders or taxpayers with the downside) has led to disasters. **We remediate this by aligning incentives** – for instance, major proposals with significant risk must be approved by those who have a stake in long-term outcomes (executives, stakeholders) to ensure no one is playing with asymmetric house money. As one commentator put it, *“the absence of skin in the game is the presence of power without responsibility or vulnerability.”* ([Having 'Skin in the Game' - Common Dreams](https://www.commondreams.org/views/2012/12/07/having-skin-game#:~:text=Having%20%27Skin%20in%20the%20Game%27,power%20without%20responsibility%20or%20vulnerability)) We strive for the opposite: with responsibility comes necessary caution.

- **“Never Risk Ruin” – Downside Protection as Strategy:** Prominent strategists and investors reinforce the idea that **avoiding catastrophic downside is priority one**. As investor Ray Dalio advises, *“Make sure that the probability of the unacceptable (i.e., the risk of ruin) is nil.”* ([Principles by Ray Dalio](https://www.principles.com/principles/b98e3caf-b72f-4d4e-a0f7-b3470f437216/#:~:text=Make%20sure%20that%20the%20probability,is%20nil)) In practice, this means **no initiative or deal should ever carry a plausible chance of bankrupting the company or permanently crippling our business** – no matter the supposed upside. The expected value of a gamble is irrelevant if it includes even a tiny probability of ruin, because **ruin = game over**. Our policy formalizes prudent risk limits: **we do not gamble the company’s future on single shots**. We prefer strategies that survive to fight another day. This mindset is echoed in business strategy research as well. In *Great by Choice*, Jim Collins describes “productive paranoia” – the best leaders continuously ask *“What if?”* and prepare for worst-case scenarios ([Productive Paranoia: Lesson #3 From Jim Collins’ Great By Choice | Stephen Blandino](https://stephenblandino.com/2012/01/productive-paranoia-lesson-3-from-jim-collins-great-by-choice.html#:~:text=Collins%20and%20Hansen%20make%20it,91)). They build buffers and **avoid recklessness**, knowing that **only the mistakes you survive can teach you lessons** ([Productive Paranoia: Lesson #3 From Jim Collins’ Great By Choice | Stephen Blandino](https://stephenblandino.com/2012/01/productive-paranoia-lesson-3-from-jim-collins-great-by-choice.html#:~:text=Collins%20and%20Hansen%20make%20it,91)). We foster a similar **paranoia about big downsides**: **plan for shocks** and never assume “it’ll never happen to us.” By being **hyper-aware of what could go wrong**, we either steer clear of those risks or mitigate them to acceptable levels. This doesn’t mean avoiding all risk (we still take prudent risks to innovate), but **we fiercely guard against tail-risk** – the kind of rare but disastrous outcome that could wipe out years of progress.

- **Asymmetric Upside (Optionality) vs. Asymmetric Downside:** We encourage thinking in terms of **asymmetry**. A good decision often has **limited downside and lots of upside** (positive asymmetry), while a bad (concave) decision is the opposite ([
Lead Time Distributions and Antifragility | Connected Knowledge](https://connected-knowledge.com/2014/09/09/lead-time-distributions-and-antifragility/#:~:text=A%20concave%20payoff%20function%20would,limited%20gains%20and%20unlimited%20losses)). Taleb and others advocate building **optionality** into ventures – small bets that could pay off big, while losses are contained ([What is Optionality? How Can Optionality Improve My Performance?](https://taylorpearson.me/optionality/#:~:text=In%20Taleb%E2%80%99s%20words%3A)) ([What is Optionality? How Can Optionality Improve My Performance?](https://taylorpearson.me/optionality/#:~:text=,%E2%80%9D)). For example, developing a prototype or running a low-cost experiment before a full rollout gives us one-sided exposure: if it fails, losses are minor; if it succeeds, we scale the benefits. This is the opposite of throwing all our resources into an untested idea (which is a **high downside gamble**). We want to **maximize opportunities for upside gain while limiting the downside risk at every step** ([What's The Barbell Strategy? - Definition, Examples, and More — Wealest](https://www.wealest.com/articles/barbell-strategy#:~:text=Summary%20of%20the%20Barbell%20Strategy)). Put simply, *“clip your downside, protect yourself from extreme harm, and let the upside take care of itself.”* ([What's The Barbell Strategy? - Definition, Examples, and More — Wealest](https://www.wealest.com/articles/barbell-strategy#:~:text=%E2%80%9CSomeone%20with%20100%C2%A0percent%20in%20so,%E2%80%9D)) By institutionalizing this principle, especially in our post-acquisition environment, we steer teams to **favor safe-to-fail experiments and iterative progress** over big-bang moves that could backfire.

- **Behavioral Biases and Traps to Avoid:** Be aware of common **decision-making biases** that tempt us into concave decisions. One is the **sunk cost fallacy** – the tendency to keep investing in a losing proposition because we’ve already spent so much on it. This often turns a contained loss into a potentially **bigger disaster**, essentially “digging ourselves into a deeper hole” ([How Sunk Cost Fallacy Influences Our Decisions [2025] • Asana](https://asana.com/resources/sunk-cost-fallacy#:~:text=The%20sunk%20cost%20fallacy%20is,for%20you%20and%20your%20team)). Our policy urges **cutting losses early** rather than throwing good money (or time) after bad. Another is **risk-seeking under loss**: psychology research (prospect theory) shows people often take **desperate gambles to avoid a sure loss**, even if that gamble could lead to a much larger loss ([Prospect Theory: An Analysis of Decision under Risk - jstor](https://www.jstor.org/stable/1914185#:~:text=Prospect%20Theory%3A%20An%20Analysis%20of,in%20choices%20involving%20sure%20losses)). In a project context, this means a manager might double-down on a failing project with an extremely risky overhaul, hoping to *win it all back*. Such behavior can convert a small failure into a catastrophe. We encourage an alternative approach: **admit small failures, learn, and move on** (which is actually how we become antifragile by learning from stressors). Additionally, **overconfidence bias** can make someone underestimate the downside (“I’m sure this risky demo will wow the client, nothing will go wrong”) – counter this by deliberately **considering worst-case scenarios** (premortem analysis: “What would be the worst thing that could happen here, and can we live with it?”). By being mindful of these biases, employees can catch themselves before they commit to a concave decision on autopilot. A culture that values **rational caution over bravado** will naturally filter out many concave risks.

In summary, the theory and insights underline a simple truth: **long-term success comes from avoiding ruinous mistakes** even more than achieving brilliant wins. By building an antifragile mindset – **benefiting from volatility when possible, but always surviving it** – we ensure the company’s longevity and health through growth and uncertainty.

## Illustrative Examples in an Enterprise Software Context

To make this policy concrete, here are a few **scenarios that illustrate concave decisions** (and preferred alternatives) in our enterprise software environment. Employees at all levels should learn to spot similar patterns in their day-to-day work:

- **Overpromising to a Customer (Sales/Account Management):** A sales executive, eager to close a deal, considers promising a critical new feature integration to a prospective client by an unrealistic date. **Upside:** Might help secure one customer’s signature this quarter. **Downside:** If the promise can’t be met, we face an angry customer, damage to our reputation, potential legal liabilities (if included in contract), and firefighting to deliver something our product isn’t ready for. This is clearly a concave move – a sliver of upside with *massive downside* if it fails. **Policy in action:** Don’t commit to deliverables or timelines that our teams haven’t vetted. Instead, take a more **convex approach**: set honest expectations (perhaps pilot a smaller feature now), or offer contractual flexibility. It’s better to lose a deal than to win it in a way that could blow up later. We want **sustainable, transparent client relationships**, not ticking time bombs. Before making bold promises, ask *“What if we’re wrong?”* If the answer is “we lose the customer’s trust and others hear about it,” don’t do it.

- **“Quick Fix” that Risks System Outage (Engineering/Operations):** An engineer notices a minor bug in the customer analytics module just before a big demo. There’s a **temptation to apply an untested hotfix directly in production** to quickly resolve it before the demo. **Upside:** If it works, the demo goes perfectly and nobody ever notices the issue – a small relief and a pat on the back for being proactive. **Downside:** The unvetted fix could have unforeseen side effects, potentially crashing our production environment or corrupting data during the live demo (or afterwards). That would not only ruin the demo but also impact all customers, leading to high-severity incidents and loss of confidence. This is a **concave decision** – minimal upside (a demo with zero glitches, which is nice but not game-changing) versus a huge downside (system outage). **Policy in action:** The engineer should **avoid rushing in a risky change**. Instead, they might find a safer workaround (e.g., hide the glitch in the demo tenant, or explain it briefly to the client and assure a safe fix is planned). Our culture should never penalize someone for *not* taking a wild gamble with production. We prefer **boring stability over dramatic fixes** when the stakes are asymmetric. As a rule, **never deploy a change that hasn’t been tested in a staging environment** unless the upside (preventing certain imminent disaster) clearly outweighs the risk – and even then, get a second opinion.

- **Architecture Shortcut with Asymmetric Risk (Engineering/Architecture):** A development team designing a new service considers using a single database instance for both primary and backup to save time (e.g., *“It’ll be simpler and we likely won’t have a failure in the short term”*). **Upside:** Slight reduction in complexity and a quicker initial launch. **Downside:** This creates a single point of failure – if that lone database goes down, **all data and service availability could be lost at once**. The potential impact – extended downtime, possible data loss, SLA breaches for all customers – is enormous, far outweighing the small convenience gained. This is a classic **fragile design choice**: it works fine in normal times but **very badly under stress** (concave response to failure modes). **Policy in action:** The team should implement proper redundancy (e.g., replication, backups in separate zones) even if it takes a bit more time. Designing for **graceful degradation** and failover might seem overly cautious during smooth sailing, but it’s exactly what saves us when things go wrong. We reward engineers for **resilience and robustness** – building systems that *“fail safe”* rather than *“fail big.”* If anyone feels pressure to cut corners in a way that introduces outsized risk, they should bring it up. Leadership will back them in favor of safety – *that’s* our culture.

- **Pushing a Buggy Release to Meet a Deadline (Product/Engineering):** Imagine a scenario where an acquired product’s code has not fully passed quality checks, but there’s pressure to unify it into our main platform by end of quarter. Releasing it on time would please management and perhaps earn short-term accolades. **Upside:** Hitting a deadline, showing quick integration progress, and maybe a minor boost in quarterly metrics. **Downside:** If the code is buggy or not well integrated, it could cause critical failures in our main platform – from security vulnerabilities to broken features for existing customers. The fallout could be many times worse than the benefit: customer support escalations, patch work, loss of user trust, even data breaches. This risk is **amplified by the acquisition context** – the new system might not be fully understood by our team (unknown unknowns). For the small upside of saying “we delivered integration in Q2,” the downside could be a scramble that hurts both the acquired product’s reputation and our core platform. **Policy in action:** Here the concave nature is clear: **delay the launch until it’s truly ready**. Communicate transparently about the reasons (we value quality over arbitrary deadlines). Perhaps release a beta to a small subset of customers to gather feedback safely. This aligns with the principle of **gradual rollout** to limit blast radius – a non-concave strategy. Leadership should reinforce that *missing a date is acceptable, but a major incident is not*. By **avoiding the big downside**, we live to fight (and ship) another day with our credibility intact.

- **Risky Behavior in Client Interactions (General):** This is more behavioral – e.g., a consultant or support rep in a customer meeting decides to **improvise beyond their authority** to placate an angry client. Perhaps they share unconfirmed information, or agree to a contract concession without approval, or make an off-color joke to lighten the mood. **Upside:** Best case, the client is momentarily satisfied or amused, and the situation slightly improves. **Downside:** The incorrect information or unauthorized promise backfires badly – the client acts on it or expects it to be true, and when the truth comes out, trust is shattered and the account (or even legal action) is at risk. Or the joke falls flat and offends the client, damaging the relationship. In all these cases, a trivial short-term gain (avoiding a few minutes of discomfort in a meeting) is not worth the major downside (jeopardizing a customer relationship or our company’s integrity). **Policy in action:** Employees should stick to the truth and approved parameters in customer interactions, even if it means holding the line in a tough conversation. If under pressure, **don’t gamble** on a lie or an unvetted commitment. It’s better to politely defer (“I’ll check with our team on that”) than to **expose the company to a promise we can’t keep**. This example underscores that **ethical and prudent conduct is non-negotiable** – reputational damage is a huge downside with no real upside. We back employees who handle such situations professionally and escalate instead of taking a dangerous gamble solo.

**Bottom line:** Across all departments – from engineering decisions and product strategy to sales tactics and operations – **always weigh the asymmetry of outcomes**. If an action puts us in a position where *“if we’re wrong, the hit is far worse than any gain if we’re right,”* that’s a **concave decision and against our policy**. Our company has thrived by delivering reliable software and earning trust; one misstep with catastrophic downside can undo years of goodwill. By diligently applying this policy and mindset, we ensure that **no single decision will sink the ship**. Instead, we focus on steady, sustainable growth, where **upsides are welcomed and downsides are contained**.

Employees are encouraged to discuss potential risks openly and **use these principles in day-to-day judgment calls**. When in doubt, remember: **protecting the company from extreme harm is everyone’s responsibility**. A culture that **avoids concave decisions** is one that stays robust under stress and keeps improving — it’s how we remain **antifragile in the long run** ([Antifragile (book) - Wikipedia](https://en.wikipedia.org/wiki/Antifragile_(book)#:~:text=Taleb%27s%20thesis%20is%20that%20in,small%20bets%20that%20have%20asymmetric)), turning uncertainties into opportunities while always safeguarding our foundation. 

